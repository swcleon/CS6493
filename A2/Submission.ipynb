{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS6493 Natural Language Processing: Assignment 2\n",
    "\n",
    "Name: Shiwei Chen \\\n",
    "UID: 56708605 \\\n",
    "Contact: shiwechen6-c@my.cityu.edu.hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 - RNN & LSTM\n",
    "\n",
    "(**20 marks**) Recurrent neural network (RNN) is a widely used model in NLP since it can process any length input. However, RNN unfortunately suffers from vanishing/exploading gradient and long-term dependency. To this end, Long Short-Term Memory (LSTM) is proposed to address these problems. The Figure 1 illustrate the architecture of LSTM.\n",
    "\n",
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/40/LSTM.jpg\" width=\"325\" height='250'>\n",
    "        <figcaption>Figure 1: Overview of LSTM.</figcaption>\n",
    "    </figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** Briefly explain why RNN suffers from vanishing/exploding gradient and long-term dependency. (**4 marks**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b.** Can LSTM solve the problem of vanishing and exploding gradient? Explain the reasons. (**4 marks**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1c.** An advantage of RNN/LSTM is that the model size does not increase for longer inputs. Given a LSTM cell where the inpyt and output dimensions are repectively $d_i$ and $d_o$, please calculate the number of parameters of this LSTM cell. (**4 marks**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** The following code snippet builds a typical classification model with LSTM. In specific, this model is comprised of an *Embedding* layer, a *LSTM* layer, and a linear $decoder*. Please calculate the total number of the parameters of this model. Provide your calculation process. (**8 marks**)\n",
    "```Python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocabs_size=1000, \n",
    "                 embedding_dim=10,\n",
    "                 hidden_size=20, \n",
    "                 num_class=5):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            bidirectional=True,\n",
    "                            num_layers=1)\n",
    "        self.decoder = nn.Linear(hidden_size, num_class, bias=True)\n",
    "\n",
    "model = MyModel()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
